{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare the Q-value estimation of Q-learning and Deep Q-Network (DQN)\n",
    "\n",
    "Use CliffWalking env, cauz it's simple and easy to understand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from reinforce.alg.reinforce import REINFORCE\n",
    "from actor_critic.alg.actor_critic import ActorCritic\n",
    "import gymnasium as gym\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def set_seed(seed):\n",
    "    \"\"\"\n",
    "    Set random seed for reproducibility.\n",
    "    \"\"\"\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def train_on_policy_agent(env, agent, num_episodes, num_pbar, state_selected, action_selected):\n",
    "    # to record episode returns\n",
    "    return_list = []\n",
    "    for i in range(num_pbar):\n",
    "        with tqdm(total=int(num_episodes/num_pbar), desc=\"Iteration %d\"%(i)) as pbar:\n",
    "            for i_episode in range(int(num_episodes/num_pbar)): # for each pbar, there are int(num_episodes/10) episodes\n",
    "                # each episode\n",
    "                episode_return = 0\n",
    "                transition_dict = {\n",
    "                    \"states\":[],\n",
    "                    \"actions\":[],\n",
    "                    \"next_states\":[],\n",
    "                    \"rewards\":[],\n",
    "                    \"dones\":[]\n",
    "                }\n",
    "                # reset environment\n",
    "                state, _ = env.reset()\n",
    "                terminated, truncated = False, False\n",
    "                while not terminated and not truncated: # interaction loop\n",
    "                    # select action\n",
    "                    action = agent.take_action(state)\n",
    "                    # step the environment\n",
    "                    next_state, reward, terminated, truncated, _ = env.step(action)\n",
    "                    # store transition\n",
    "                    transition_dict[\"states\"].append(state)\n",
    "                    transition_dict[\"actions\"].append(action)\n",
    "                    transition_dict[\"next_states\"].append(next_state)\n",
    "                    transition_dict[\"rewards\"].append(reward)\n",
    "                    transition_dict[\"dones\"].append(terminated)\n",
    "                    # update state\n",
    "                    state = next_state\n",
    "                    episode_return += reward\n",
    "                # episode finished\n",
    "                return_list.append(episode_return)\n",
    "                # update agent\n",
    "                agent.update(transition_dict)\n",
    "                # show mean return of last 10 episodes, every 10 episodes\n",
    "                if (i_episode+1) % 10 == 0:\n",
    "                    pbar.set_postfix({\n",
    "                        \"episode\":\n",
    "                        \"%d\" % (int(num_episodes/10)*i+i_episode+1),\n",
    "                        \"return\":\n",
    "                        \"%.3f\" % (np.mean(return_list[-10:]))\n",
    "                    })\n",
    "                # update pbar\n",
    "                pbar.update(1)\n",
    "    # return return_list\n",
    "    return return_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# qlearning params\n",
    "lr = 1e-3\n",
    "gamma = 0.99\n",
    "# DQN params\n",
    "actor_lr = 1e-4 # alpha in q-learning is replaced by lr in DQN\n",
    "critic_lr = 2e-3\n",
    "hidden_dim = 128  # number of neurons in the hidden layer\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")  # use GPU if available\n",
    "# env params\n",
    "env_name = 'CartPole-v1'  # name of the environment to train on\n",
    "# training params\n",
    "num_pbar = 1 # number of progress bar\n",
    "num_episodes = 100 # number of episodes to run\n",
    "seed = 0  # random seed for reproducibility\n",
    "\n",
    "# define the environment and the agent\n",
    "env = gym.make(env_name)\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "state_selected, _ = env.reset()\n",
    "action_selected = 1\n",
    "vpg_agent = REINFORCE(state_dim, hidden_dim, action_dim, state_selected, action_selected, lr, gamma, device, \"discrete\")\n",
    "a2c_agent = ActorCritic(state_dim, hidden_dim, action_dim, actor_lr, critic_lr, gamma, device, \"discrete\")\n",
    "set_seed(seed)\n",
    "# train the agent\n",
    "\n",
    "\n",
    "\n",
    "# plot the results\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12, 6))\n",
    "ax[0].plot(dqn_return_list, label=\"DQN\")\n",
    "ax[0].plot(ql_return_list, label=\"Q-learning\")\n",
    "ax[0].set_xlabel(\"Episode\")\n",
    "ax[0].set_ylabel(\"Return\")\n",
    "ax[0].set_title(\"Comparison of DQN and Q-learning\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(dqn_q_list, label=\"DQN\")\n",
    "ax[1].plot(ql_q_list, label=\"Q-learning\")\n",
    "ax[1].set_xlabel(\"Episode\")\n",
    "ax[1].set_ylabel(\"Q-value\")\n",
    "ax[1].set_title(\"Comparison of DQN and Q-learning\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在CliffWalking环境下训练，DQN训练很慢，每个episode训练时间很长，可能的原因想到了2个：\n",
    "1. 神经网络本身就是用于浮点数(连续空间)的运算，强行将整型转为浮点来算Q值可能并不准确.\n",
    "2. 浮点数本身有误差，在同样的网络参数下同一个整型的state输入进去可能得到的Q值估计会有差别，即使差别很小，但如果不同动作的Q值大小关系发生了变化，策略就会发生很大的变化，导致训练不稳定。\n",
    "3. 一般DQN会用DL的方法从数据库中采样数据batch，然后更新网络。这么做需要先有一定的数据，在数据量还不够时，策略就会很盲目，类似MonteCarlo方法在更新前的状态，一直往上走，导致奖励很小，估计误差很大，可能直接就毁了训练。而将batch_size设为1，实际上效果也不好。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_gym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
