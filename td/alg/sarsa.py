import numpy as np

class SARSA:

    def __init__(self, env, epsilon, alpha, gamma):
        self.env = env
        # gym only defines observation space but not num_obs
        try:
            self.num_obs = self.env.observation_space.n
        except: # for blackjack env
            obs_space = self.env.observation_space 
            self.num_obs = 1
            for obs in obs_space:
                self.num_obs *= obs.n
        self.num_action = self.env.action_space.n
        self.Q_table = np.zeros([self.num_obs, self.num_action]) # init Q(s,a) table
        self.epsilon = epsilon # epsilon in epsilon-greedy policy
        self.alpha = alpha # learning rate
        self.gamma = gamma # discount factor
    
    def take_action(self, obs):  # epsilon-greedy policy
        if np.random.random() < self.epsilon:
            action = np.random.randint(self.num_action) # encourage exploration
        else:
            action = np.argmax(self.Q_table[obs])
        return action
    
    def best_action(self, obs):  # For print policy
        Q_max = np.max(self.Q_table[obs])
        a = [0 for _ in range(self.num_action)]
        for i in range(self.num_action):  # If multiple actions have the same Q value, they will all be selected.
            if self.Q_table[obs, i] == Q_max:
                a[i] = 1
        return a
    
    def update(self, s0, a0, r, s1, a1): # update Q table, using the Q value generated by epsilon-greedy policy(on-policy).
        td_error = r + self.gamma * self.Q_table[s1, a1] - self.Q_table[s0, a0]
        self.Q_table[s0, a0] += self.alpha * td_error