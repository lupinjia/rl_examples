# Folder and File Explanation

- **alg**: Contains the implementation of SAC algorithm.
- **pendulum.py**: SAC training example on the Pendulum environment of OpenAI Gymnasium.

# Algorithm Introduction

Soft Actor-Critic (SAC) is a state-of-the-art off-policy reinforcement learning algorithm designed for environments with continuous action spaces. It was introduced by Tuomas Haarnoja, Aurick Zhou, and Pieter Abbeel in a paper titled "Soft Actor-Critic: Off-Policy Maximum Entropy Deep Reinforcement Learning with a Stochastic Actor." SAC builds upon and extends the ideas from the Actor-Critic framework and the maximum entropy reinforcement learning paradigm.

You can find more information about SAC algorithm in the following resources:

- [Hands on RL](https://hrl.boyuai.com/chapter/2/sac%E7%AE%97%E6%B3%95)

# Additional Notes

- The provided examples are designed for educational purposes, illustrating the core concepts of TD learning.
- Users are encouraged to modify and extend these examples to explore different aspects of TD learning and its applications.